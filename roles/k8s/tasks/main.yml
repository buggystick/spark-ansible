---
# roles/k8s/tasks/main.yml
# Idempotent Kubernetes (kubeadm) setup using containerd (CRI enabled)
# - Enables CRI + SystemdCgroup in containerd
# - Optional full reset via -e k8s_reset=true
# - Auto-rescues kubeadm init if it fails
# - Joins workers only if not already joined
# - Installs Cilium via Helm using admin.conf
# Inventory groups expected: [primary], [workers]

# ------- Defaults (override via -e) -------
- name: Set sensible defaults
  set_fact:
    k8s_pod_cidr: "{{ k8s_pod_cidr | default('10.244.0.0/16') }}"
    k8s_cri_socket: "{{ k8s_cri_socket | default('unix:///run/containerd/containerd.sock') }}"
    k8s_channel: "{{ k8s_channel | default('v1.30') }}"
    k8s_cilium_chart_version: "{{ k8s_cilium_chart_version | default('1.18.3') }}"
    k8s_traefik_chart_version: "{{ k8s_traefik_chart_version | default('37.2.0') }}"

# ------- Base OS deps -------
- name: Install base packages
  apt:
    name:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg
      - lsb-release
      - python3-kubernetes
    state: present
    update_cache: yes

# ------- Kubernetes APT repo + packages -------
- name: Ensure /etc/apt/keyrings exists
  ansible.builtin.file:
    path: /etc/apt/keyrings
    state: directory
    mode: '0755'

- name: Add Kubernetes apt signing key
  ansible.builtin.apt_key:
    url: "https://pkgs.k8s.io/core:/stable:/{{ k8s_channel }}/deb/Release.key"
    state: present
    keyring: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

- name: Add Kubernetes apt repository
  ansible.builtin.apt_repository:
    repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/{{ k8s_channel }}/deb/ /"
    filename: kubernetes
    state: present
    update_cache: yes

- name: Install kubelet, kubeadm, kubectl
  apt:
    name:
      - kubelet
      - kubeadm
      - kubectl
    state: present
    update_cache: yes


# ------- Containerd + CRI (known-good config) -------
- name: Install containerd and cri-tools
  apt:
    name:
      - containerd
      - cri-tools
    state: present

- name: Ensure /etc/containerd exists
  file:
    path: /etc/containerd
    state: directory
    mode: '0755'

- name: Push known-good containerd config (CRI enabled + SystemdCgroup)
  copy:
    dest: /etc/containerd/config.toml
    mode: '0644'
    content: |
      version = 2
      imports = ["/etc/containerd/conf.d/*.toml"]

      root = "/var/lib/containerd"
      state = "/run/containerd"

      [grpc]

      [plugins]
        [plugins."io.containerd.grpc.v1.cri"]
          sandbox_image = "registry.k8s.io/pause:3.9"
          [plugins."io.containerd.grpc.v1.cri".containerd]
            snapshotter = "overlayfs"
            [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
              [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                runtime_type = "io.containerd.runc.v2"
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                  SystemdCgroup = true

      [debug]
        level = "info"

- name: Ensure crictl talks to containerd
  copy:
    dest: /etc/crictl.yaml
    mode: '0644'
    content: |
      runtime-endpoint: {{ k8s_cri_socket }}
      image-endpoint:   {{ k8s_cri_socket }}
      timeout: 10
      debug: false

- name: Kernel modules for Kubernetes
  copy:
    dest: /etc/modules-load.d/k8s.conf
    content: |
      br_netfilter
      overlay

- name: Sysctl for Kubernetes
  copy:
    dest: /etc/sysctl.d/99-k8s.conf
    content: |
      net.bridge.bridge-nf-call-iptables = 1
      net.bridge.bridge-nf-call-ip6tables = 1
      net.ipv4.ip_forward = 1

- name: Ensure overlay module is loaded
  community.general.modprobe:
    name: overlay
    state: present

- name: Ensure br_netfilter module is loaded
  community.general.modprobe:
    name: br_netfilter
    state: present

- name: Ensure netfilter sysctl for IPv4 is set
  ansible.posix.sysctl:
    name: net.bridge.bridge-nf-call-iptables
    value: '1'
    state: present
    sysctl_set: true
    reload: true

- name: Ensure netfilter sysctl for IPv6 is set
  ansible.posix.sysctl:
    name: net.bridge.bridge-nf-call-ip6tables
    value: '1'
    state: present
    sysctl_set: true
    reload: true

- name: Ensure IPv4 forwarding is enabled
  ansible.posix.sysctl:
    name: net.ipv4.ip_forward
    value: '1'
    state: present
    sysctl_set: true
    reload: true

- name: Restart & enable containerd
  systemd:
    name: containerd
    state: restarted
    enabled: yes

# ------- Swap off (idempotent) -------
- name: Disable swap now (if any)
  command: swapoff -a
  when: ansible_swaptotal_mb|default(0) | int > 0
  changed_when: ansible_swaptotal_mb|default(0) | int > 0

- name: Comment swap lines in fstab
  replace:
    path: /etc/fstab
    regexp: '^([^#].*\\sswap\\s.*)$'
    replace: '# \\1'

- name: Enable kubelet
  systemd:
    name: kubelet
    enabled: yes

# ------- Optional global reset (if requested) -------
- name: Reset Kubernetes (kubeadm) if requested
  when: k8s_reset | default(false) | bool
  block:
    - name: Stop kubelet (prevent new pods/mounts)
      systemd:
        name: kubelet
        state: stopped
        enabled: yes
      failed_when: false

    - name: Force-clean containerd k8s workload (tasks, containers, CRI pods)
      shell: |
        set -Eeuo pipefail
        removed_any=0

        # Kill/delete any running tasks in containerd k8s namespace
        for id in $(ctr -n k8s.io tasks ls -q 2>/dev/null || true); do
          ctr -n k8s.io tasks kill -s KILL "$id" 2>/dev/null || true
          ctr -n k8s.io tasks delete "$id" 2>/dev/null || true
          removed_any=1
        done

        # Delete any containers in containerd k8s namespace
        for id in $(ctr -n k8s.io containers ls -q 2>/dev/null || true); do
          ctr -n k8s.io containers delete "$id" 2>/dev/null || true
          removed_any=1
        done

        # Remove CRI containers/pods if CRI still lists any (safe if none)
        if crictl ps -aq >/dev/null 2>&1; then
          conts="$(crictl ps -aq || true)"
          if [ -n "${conts:-}" ]; then
            crictl rm --force $conts 2>/dev/null || true
            removed_any=1
          fi
        fi
        if crictl pods -aq >/dev/null 2>&1; then
          pods="$(crictl pods -aq || true)"
          if [ -n "${pods:-}" ]; then
            crictl rmp --force $pods 2>/dev/null || true
            removed_any=1
          fi
        fi

        # Lazy-unmount any kubelet mounts
        mnts="$(mount | awk '/\/var\/lib\/kubelet/ {print $3}' || true)"
        if [ -n "${mnts:-}" ]; then
          umount -l ${mnts} 2>/dev/null || true
          removed_any=1
        fi

        # Print a marker if anything was removed (used for changed_when)
        if [ "$removed_any" -eq 1 ]; then
          echo "CLEANED_CONTAINERD_OR_MOUNTS"
        fi
      args:
        executable: /bin/bash
      register: _k8s_cleanup_containerd
      changed_when: "'CLEANED_CONTAINERD_OR_MOUNTS' in _k8s_cleanup_containerd.stdout"
      failed_when: false

    - name: Restart containerd to refresh CRI
      systemd:
        name: containerd
        state: restarted
        enabled: yes
      failed_when: false

    - name: kubeadm reset (explicit CRI socket)
      command: kubeadm reset -f --cri-socket=unix:///run/containerd/containerd.sock
      register: _reset
      failed_when: false

    - name: Clean CNI state (safe to remove)
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/cni/net.d
        - /var/lib/cni
        - /var/run/cni
        - /var/lib/cilium

    # /var/run/cilium can contain protected cgroup files (e.g., io.prio.class).
    # Try to unmount (if bind-mounted), then best-effort prune contents and remove dir.
    - name: Clean /var/run/cilium safely
      shell: |
        set -Eeuo pipefail
        if mountpoint -q /var/run/cilium; then
          umount -l /var/run/cilium || true
        fi
        if [ -d /var/run/cilium ]; then
          # Remove what we can; ignore protected entries
          find /var/run/cilium -mindepth 1 -exec rm -rf -- {} + 2>/dev/null || true
          rmdir /var/run/cilium 2>/dev/null || true
        fi
      args:
        executable: /bin/bash
      changed_when: false
      failed_when: false

    - name: Recreate CNI dir
      file:
        path: /etc/cni/net.d
        state: directory
        mode: '0755'

# ------- Control plane (primary) with auto-rescue -------
- name: Initialize control-plane (primary only) with auto-rescue
  block:
    - name: Check if control-plane already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: k8s_adminconf

    - name: kubeadm init (only if not initialized)
      command: >
        kubeadm init
        --pod-network-cidr={{ k8s_pod_cidr }}
        --cri-socket={{ k8s_cri_socket }}
      when: not k8s_adminconf.stat.exists

  rescue:
    - name: Force reset control-plane (rescue)
      command: kubeadm reset -f
    - name: kubeadm init (retry after reset)
      command: >
        kubeadm init
        --pod-network-cidr={{ k8s_pod_cidr }}
        --cri-socket={{ k8s_cri_socket }}
  when: "'primary' in group_names"

- name: Ensure user kube dir exists (primary only)
  file:
    path: "/home/{{ ansible_user }}/.kube"
    state: directory
    mode: '0755'
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
  when: "'primary' in group_names"

- name: Copy kubeconfig to user (primary only)
  copy:
    remote_src: true
    src: /etc/kubernetes/admin.conf
    dest: "/home/{{ ansible_user }}/.kube/config"
    mode: '0644'
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
  when: "'primary' in group_names"

# ------- Wait for API before Helm-based installs -------
- name: Wait for API server by listing default Namespace (primary only)
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Namespace
    name: default
    kubeconfig: /etc/kubernetes/admin.conf
  register: _apiready
  retries: 30
  delay: 2
  until: _apiready.resources is defined and (_apiready.resources | length) > 0
  changed_when: false
  when: "'primary' in group_names"

# ------- Generate join command on primary & share to workers -------
- name: Obtain join command (primary only)
  command: kubeadm token create --print-join-command --ttl 30m
  register: _join_cmd_raw
  changed_when: false
  when: "'primary' in group_names"

- name: Build final join command with CRI socket (primary only)
  set_fact:
    k8s_join_cmd_final: "{{ _join_cmd_raw.stdout }} --cri-socket {{ k8s_cri_socket }}"
  when: "'primary' in group_names"

- name: Share join command to all hosts
  set_fact:
    cluster_join_cmd: "{{ hostvars[groups['primary'][0]].k8s_join_cmd_final | default('') }}"
  when: groups['primary'] | length > 0

# ------- Workers join idempotently -------
- name: Check if worker already joined
  stat:
    path: /etc/kubernetes/kubelet.conf
  register: _worker_joined
  when: "'workers' in group_names"

- name: Join worker to cluster (if not joined)
  command: "{{ cluster_join_cmd }}"
  when:
    - "'workers' in group_names"
    - not _worker_joined.stat.exists
    - cluster_join_cmd is defined
    - cluster_join_cmd | length > 0

# ------- CNI (Cilium via Helm using admin.conf) -------
- name: Install Helm (primary only)
  shell: curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  args:
    creates: /usr/local/bin/helm
  when: "'primary' in group_names"

- name: Ensure Cilium Helm repository is configured (primary only)
  kubernetes.core.helm_repository:
    name: cilium
    repo_url: https://helm.cilium.io
    state: present
    force_update: true
    kubeconfig: /etc/kubernetes/admin.conf
  when: "'primary' in group_names"

- name: Deploy Cilium via Helm (primary only)
  kubernetes.core.helm:
    name: cilium
    chart_ref: cilium
    chart_repo_url: https://helm.cilium.io
    chart_version: "{{ k8s_cilium_chart_version }}"
    release_namespace: kube-system
    create_namespace: false
    kubeconfig: /etc/kubernetes/admin.conf
    wait: true
    wait_timeout: "600s"
    state: present
  when: "'primary' in group_names"

- name: Wait for Cilium pods to be ready (primary only)
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Pod
    namespace: kube-system
    label_selectors:
      - k8s-app=cilium
    kubeconfig: /etc/kubernetes/admin.conf
    wait: true
    wait_sleep: 10
    wait_timeout: 300
    wait_condition:
      type: Ready
      status: "True"
  when: "'primary' in group_names"

# ------- Traefik Ingress Controller (primary only) -------
- name: Install Traefik ingress controller (primary only)
  when:
    - "'primary' in group_names"
    - k8s_install_traefik | default(true) | bool
  block:
    - name: Ensure Traefik Helm repository is configured
      kubernetes.core.helm_repository:
        name: traefik
        repo_url: https://traefik.github.io/charts
        state: present
        force_update: true
        kubeconfig: /etc/kubernetes/admin.conf

    - name: Create Traefik namespace
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ traefik_namespace }}"
        state: present
        kubeconfig: /etc/kubernetes/admin.conf

    - name: Copy Traefik Helm values
      copy:
        src: traefik-values.yaml
        dest: /tmp/traefik-values.yaml
        mode: '0644'

    - name: Install/upgrade Traefik via Helm
      kubernetes.core.helm:
        name: traefik
        chart_ref: traefik
        chart_repo_url: https://traefik.github.io/charts
        chart_version: "{{ (traefik_chart_version | default('') | length > 0) | ternary(traefik_chart_version, k8s_traefik_chart_version) }}"
        release_namespace: "{{ traefik_namespace }}"
        create_namespace: false
        values_files:
          - /tmp/traefik-values.yaml
        kubeconfig: /etc/kubernetes/admin.conf
        wait: true
        wait_timeout: "600s"
        state: present

    - name: Wait for Traefik deployment to be ready
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: Deployment
        namespace: "{{ traefik_namespace }}"
        name: traefik
        kubeconfig: /etc/kubernetes/admin.conf
        wait: true
        wait_sleep: 10
        wait_timeout: 300
        wait_condition:
          type: Available
          status: "True"
