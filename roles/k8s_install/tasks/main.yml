---
# roles/k8s_install/tasks/main.yml
# Split from the old roles/k8s role: only install/configure logic lives here.
# Uses containerd as CRI, kubeadm for control plane/worker join, and installs Cilium and optional Traefik.

# ------- Defaults (override via -e) -------
- name: Set sensible defaults
  set_fact:
    k8s_pod_cidr: "{{ k8s_pod_cidr | default('10.244.0.0/16') }}"
    k8s_cri_socket: "{{ k8s_cri_socket | default('unix:///run/containerd/containerd.sock') }}"
    k8s_channel: "{{ k8s_channel | default('v1.30') }}"
    k8s_cilium_chart_version: "{{ k8s_cilium_chart_version | default('1.18.3') }}"
    k8s_traefik_chart_version: "{{ k8s_traefik_chart_version | default('37.2.0') }}"

# ------- Base OS deps -------
- name: Install base packages
  apt:
    name:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg
      - lsb-release
      - python3-kubernetes
    state: present
    update_cache: yes

# ------- Containerd + CRI (known-good config) -------
- name: Install containerd and cri-tools
  apt:
    name:
      - containerd
      - cri-tools
    state: present

- name: Ensure /etc/containerd exists
  file:
    path: /etc/containerd
    state: directory
    mode: '0755'

- name: Push known-good containerd config (CRI enabled + SystemdCgroup)
  copy:
    dest: /etc/containerd/config.toml
    mode: '0644'
    content: |
      version = 2
      imports = ["/etc/containerd/conf.d/*.toml"]

      root = "/var/lib/containerd"
      state = "/run/containerd"

      [grpc]

      [plugins]
        [plugins."io.containerd.grpc.v1.cri"]
          sandbox_image = "registry.k8s.io/pause:3.9"
          [plugins."io.containerd.grpc.v1.cri".containerd]
            snapshotter = "overlayfs"
            [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
              [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                runtime_type = "io.containerd.runc.v2"
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                  SystemdCgroup = true

      [debug]
        level = "info"

- name: Ensure crictl talks to containerd
  copy:
    dest: /etc/crictl.yaml
    mode: '0644'
    content: |
      runtime-endpoint: {{ k8s_cri_socket }}
      image-endpoint:   {{ k8s_cri_socket }}
      timeout: 10
      debug: false

- name: Kernel modules for Kubernetes
  copy:
    dest: /etc/modules-load.d/k8s.conf
    content: |
      br_netfilter
      overlay

- name: Sysctl for Kubernetes
  copy:
    dest: /etc/sysctl.d/99-k8s.conf
    content: |
      net.bridge.bridge-nf-call-iptables = 1
      net.bridge.bridge-nf-call-ip6tables = 1
      net.ipv4.ip_forward = 1

- name: Ensure overlay module is loaded
  community.general.modprobe:
    name: overlay
    state: present

- name: Ensure br_netfilter module is loaded
  community.general.modprobe:
    name: br_netfilter
    state: present

- name: Ensure netfilter sysctl for IPv4 is set
  ansible.posix.sysctl:
    name: net.bridge.bridge-nf-call-iptables
    value: '1'
    state: present
    sysctl_set: true
    reload: true

- name: Ensure netfilter sysctl for IPv6 is set
  ansible.posix.sysctl:
    name: net.bridge.bridge-nf-call-ip6tables
    value: '1'
    state: present

- name: Ensure IPv4 forwarding sysctl is set
  ansible.posix.sysctl:
    name: net.ipv4.ip_forward
    value: '1'
    state: present

- name: Ensure containerd service is enabled & started
  systemd:
    name: containerd
    enabled: true
    state: started

# ------- Kubernetes apt repo + packages -------
- name: Ensure /etc/apt/keyrings exists
  file:
    path: /etc/apt/keyrings
    state: directory
    mode: '0755'

- name: Add Kubernetes apt key (signed repository)
  get_url:
    url: "https://pkgs.k8s.io/core:/stable:/{{ k8s_channel }}/deb/Release.key"
    dest: /etc/apt/keyrings/kubernetes-apt-keyring.gpg
    mode: '0644'

- name: Add Kubernetes apt repository
  apt_repository:
    repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/{{ k8s_channel }}/deb/ /"
    filename: kubernetes
    state: present
    update_cache: yes

- name: Install kubelet, kubeadm, kubectl
  apt:
    name:
      - kubelet
      - kubeadm
      - kubectl
    state: present
    update_cache: yes

- name: Enable kubelet (do not start yet)
  systemd:
    name: kubelet
    enabled: true
    state: stopped

# ------- Primary: kubeadm init with retry and recovery -------
- name: Check if control plane is already initialized (primary only)
  stat:
    path: /etc/kubernetes/admin.conf
  register: _cp_inited
  when: "'primary' in group_names"

- name: Generate kubeadm init config (primary only)
  copy:
    dest: /tmp/kubeadm-init.yaml
    mode: '0644'
    content: |
      apiVersion: kubeadm.k8s.io/v1beta4
      kind: ClusterConfiguration
      networking:
        podSubnet: {{ k8s_pod_cidr }}
      kubernetesVersion: stable
      apiServer:
        extraArgs:
          authorization-mode: Node,RBAC
  when: "'primary' in group_names and not _cp_inited.stat.exists"

- name: Attempt kubeadm init (primary only)
  command: kubeadm init --config /tmp/kubeadm-init.yaml --cri-socket {{ k8s_cri_socket }}
  register: _init_try
  changed_when: true
  failed_when: false
  when: "'primary' in group_names and not _cp_inited.stat.exists"

- name: Rescue kubeadm init failure by cleanup and retry (primary only)
  when:
    - "'primary' in group_names"
    - not _cp_inited.stat.exists
    - _init_try.rc != 0
  block:
    - name: Force-clean containerd/CRI workloads and kubelet mounts
      include_tasks: _cleanup_cri.yml
      vars:
        track_changes: false
    - name: kubeadm reset after failure
      command: kubeadm reset -f --cri-socket={{ k8s_cri_socket }}
      failed_when: false
    - name: Retry kubeadm init
      command: kubeadm init --config /tmp/kubeadm-init.yaml --cri-socket {{ k8s_cri_socket }}

- name: Ensure kubeconfig for root and ansible_user (primary only)
  when: "'primary' in group_names"
  block:
    - name: Create .kube dirs
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - /root/.kube
        - "/home/{{ ansible_user }}/.kube"
    - name: Copy admin.conf to kubeconfigs
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ item }}"
        remote_src: true
      loop:
        - /root/.kube/config
        - "/home/{{ ansible_user }}/.kube/config"

# ------- Discover and share join command -------
- name: Obtain join command from control plane (primary only)
  command: kubeadm token create --print-join-command
  register: _join_cmd_raw
  changed_when: false
  when: "'primary' in group_names"

- name: Compute final join command with CRI socket (primary only)
  set_fact:
    k8s_join_cmd_final: "{{ _join_cmd_raw.stdout }} --cri-socket {{ k8s_cri_socket }}"
  when: "'primary' in group_names"

- name: Share join command to all hosts
  set_fact:
    cluster_join_cmd: "{{ hostvars[groups['primary'][0]].k8s_join_cmd_final | default('') }}"
  when: groups['primary'] | length > 0

# ------- Workers join idempotently -------
- name: Check if worker already joined
  stat:
    path: /etc/kubernetes/kubelet.conf
  register: _worker_joined
  when: "'workers' in group_names"

- name: Join worker to cluster (if not joined)
  command: "{{ cluster_join_cmd }}"
  when:
    - "'workers' in group_names"
    - not _worker_joined.stat.exists
    - cluster_join_cmd is defined
    - cluster_join_cmd | length > 0

# ------- CNI (Cilium via Helm using admin.conf) -------
- name: Ensure Helm installer script is present (primary only)
  copy:
    src: get-helm-3.sh
    dest: /tmp/get-helm-3.sh
    mode: '0755'
  when: "'primary' in group_names"

- name: Install Helm (primary only)
  command: bash /tmp/get-helm-3.sh
  args:
    creates: /usr/local/bin/helm
  when: "'primary' in group_names"

- name: Ensure Cilium Helm repository is configured (primary only)
  kubernetes.core.helm_repository:
    name: cilium
    repo_url: https://helm.cilium.io
    state: present
    force_update: true
    kubeconfig: /etc/kubernetes/admin.conf
  when: "'primary' in group_names"

- name: Deploy Cilium via Helm (primary only)
  kubernetes.core.helm:
    name: cilium
    chart_ref: cilium
    chart_repo_url: https://helm.cilium.io
    chart_version: "{{ k8s_cilium_chart_version }}"
    release_namespace: kube-system
    create_namespace: false
    kubeconfig: /etc/kubernetes/admin.conf
    wait: true
    wait_timeout: "600s"
    state: present
  when: "'primary' in group_names"

- name: Wait for Cilium pods to be ready (primary only)
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Pod
    namespace: kube-system
    label_selectors:
      - k8s-app=cilium
    kubeconfig: /etc/kubernetes/admin.conf
    wait: true
    wait_sleep: 10
    wait_timeout: 300
    wait_condition:
      type: Ready
      status: "True"
  when: "'primary' in group_names"

# ------- Traefik Ingress Controller (primary only) -------
- name: Install Traefik ingress controller (primary only)
  when:
    - "'primary' in group_names"
    - k8s_install_traefik | default(true) | bool
  block:
    - name: Ensure Traefik Helm repository is configured
      kubernetes.core.helm_repository:
        name: traefik
        repo_url: https://traefik.github.io/charts
        state: present
        force_update: true
        kubeconfig: /etc/kubernetes/admin.conf

    - name: Create Traefik namespace
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ traefik_namespace }}"
        state: present
        kubeconfig: /etc/kubernetes/admin.conf

    - name: Copy Traefik Helm values
      copy:
        src: traefik-values.yaml
        dest: /tmp/traefik-values.yaml
        mode: '0644'

    - name: Install/upgrade Traefik via Helm
      kubernetes.core.helm:
        name: traefik
        chart_ref: traefik
        chart_repo_url: https://traefik.github.io/charts
        chart_version: "{{ traefik_chart_version }}"
        release_namespace: "{{ traefik_namespace }}"
        create_namespace: false
        values_files:
          - /tmp/traefik-values.yaml
        kubeconfig: /etc/kubernetes/admin.conf
        wait: true
        wait_timeout: "600s"
        state: present

    - name: Wait for Traefik deployment to be ready
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: Deployment
        namespace: "{{ traefik_namespace }}"
        name: traefik
        kubeconfig: /etc/kubernetes/admin.conf
        wait: true
        wait_sleep: 10
        wait_timeout: 600
        wait_condition:
          type: Available
          status: "True"
