---
# roles/nvidia_net_operator/tasks/main.yml
# Idempotent install/upgrade of NVIDIA Network Operator via Helm, using admin.conf
# Optionally applies a minimal NicClusterPolicy if CRD exists and no policy present.

- name: Ensure Helm installer script is present (primary only)
  copy:
    src: get-helm-3.sh
    dest: /tmp/get-helm-3.sh
    mode: '0755'
  when: "'primary' in group_names"

- name: Install Helm (primary only)
  command: bash /tmp/get-helm-3.sh
  args:
    creates: /usr/local/bin/helm
  when: "'primary' in group_names"

- name: Wait for API server by listing default Namespace (primary only)
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Namespace
    name: default
    kubeconfig: /etc/kubernetes/admin.conf
  register: _apiready
  retries: 30
  delay: 2
  until: _apiready.resources is defined and (_apiready.resources | length) > 0
  changed_when: false
  when: "'primary' in group_names"

- name: Ensure NVIDIA Network Operator Helm repo configured
  kubernetes.core.helm_repository:
    name: nvidia-network-operator
    repo_url: https://mellanox.github.io/network-operator
    state: present
    force_update: true
    kubeconfig: /etc/kubernetes/admin.conf
  when: "'primary' in group_names"

- name: Install/upgrade Network Operator via Helm
  kubernetes.core.helm:
    name: network-operator
    chart_ref: network-operator
    chart_repo_url: https://helm.ngc.nvidia.com/nvidia
    chart_version: "{{ network_operator_chart_version | default(omit) }}"
    release_namespace: network-operator
    create_namespace: true
    kubeconfig: /etc/kubernetes/admin.conf
    wait: true
    wait_timeout: "600s"
    state: present
  when: "'primary' in group_names"

- name: Wait for Network Operator deployment to be Available
  kubernetes.core.k8s_info:
    api_version: apps/v1
    kind: Deployment
    namespace: network-operator
    name: network-operator
    kubeconfig: /etc/kubernetes/admin.conf
    wait: true
    wait_sleep: 10
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
  when: "'primary' in group_names"

# --- Optional: create a minimal NicClusterPolicy to enable RDMA & device plugins ---
- name: Check if NicClusterPolicy CRD exists
  kubernetes.core.k8s_info:
    api_version: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nicclusterpolicies.mellanox.com
    kubeconfig: /etc/kubernetes/admin.conf
  register: _ncp_crd
  failed_when: false
  when: "'primary' in group_names"

- name: Check if a NicClusterPolicy already exists
  kubernetes.core.k8s_info:
    api_version: mellanox.com/v1alpha1
    kind: NicClusterPolicy
    kubeconfig: /etc/kubernetes/admin.conf
  register: _ncp_exists
  failed_when: false
  when: "'primary' in group_names"

- name: Gather NicClusterPolicy variable completeness information
  set_fact:
    nvidia_net_operator_missing_nicclusterpolicy_vars: "{{ _ncp_required_vars | selectattr('value', 'equalto', '') | map(attribute='name') | list }}"
  vars:
    _ncp_required_vars:
      - name: nvidia_net_ofed_repository
        value: "{{ (nvidia_net_ofed_repository | default('')) | string | trim }}"
      - name: nvidia_net_ofed_image
        value: "{{ (nvidia_net_ofed_image | default('')) | string | trim }}"
      - name: nvidia_net_ofed_version
        value: "{{ (nvidia_net_ofed_version | default('')) | string | trim }}"
      - name: nvidia_net_device_plugin_repository
        value: "{{ (nvidia_net_device_plugin_repository | default('')) | string | trim }}"
      - name: nvidia_net_device_plugin_image
        value: "{{ (nvidia_net_device_plugin_image | default('')) | string | trim }}"
      - name: nvidia_net_device_plugin_version
        value: "{{ (nvidia_net_device_plugin_version | default('')) | string | trim }}"
      - name: nvidia_net_rdma_sdp_repository
        value: "{{ (nvidia_net_rdma_sdp_repository | default('')) | string | trim }}"
      - name: nvidia_net_rdma_sdp_image
        value: "{{ (nvidia_net_rdma_sdp_image | default('')) | string | trim }}"
      - name: nvidia_net_rdma_sdp_version
        value: "{{ (nvidia_net_rdma_sdp_version | default('')) | string | trim }}"
      - name: nvidia_net_sriov_dp_repository
        value: "{{ (nvidia_net_sriov_dp_repository | default('')) | string | trim }}"
      - name: nvidia_net_sriov_dp_image
        value: "{{ (nvidia_net_sriov_dp_image | default('')) | string | trim }}"
      - name: nvidia_net_sriov_dp_version
        value: "{{ (nvidia_net_sriov_dp_version | default('')) | string | trim }}"
  when:
    - "'primary' in group_names"
    - nvidia_net_operator_apply_nicclusterpolicy | bool
  changed_when: false

- name: Fail when NicClusterPolicy variables are missing but apply is enabled
  fail:
    msg: >-
      nvidia_net_operator_apply_nicclusterpolicy is true but the following variables are empty:
      {{ (nvidia_net_operator_missing_nicclusterpolicy_vars | default([])) | join(', ') }}.
      Provide valid image repository, image, and version values or disable NicClusterPolicy auto-apply.
  when:
    - "'primary' in group_names"
    - nvidia_net_operator_apply_nicclusterpolicy | bool
    - (nvidia_net_operator_missing_nicclusterpolicy_vars | default([])) | length > 0

- name: Apply NicClusterPolicy (enable OFED, RDMA, device plugins) if none exists and enabled
  kubernetes.core.k8s:
    kubeconfig: /etc/kubernetes/admin.conf
    state: present
    definition:
      apiVersion: mellanox.com/v1alpha1
      kind: NicClusterPolicy
      metadata:
        name: nic-cluster-policy
      spec:
        ofedDriver:
          deploy: true
          repository: "{{ nvidia_net_ofed_repository }}"
          image: "{{ nvidia_net_ofed_image }}"
          version: "{{ nvidia_net_ofed_version }}"
        devicePlugin:
          deploy: true
          repository: "{{ nvidia_net_device_plugin_repository }}"
          image: "{{ nvidia_net_device_plugin_image }}"
          version: "{{ nvidia_net_device_plugin_version }}"
        rdmaSharedDevicePlugin:
          deploy: true
          repository: "{{ nvidia_net_rdma_sdp_repository }}"
          image: "{{ nvidia_net_rdma_sdp_image }}"
          version: "{{ nvidia_net_rdma_sdp_version }}"
        sriovDevicePlugin:
          deploy: false
          repository: "{{ nvidia_net_sriov_dp_repository }}"
          image: "{{ nvidia_net_sriov_dp_image }}"
          version: "{{ nvidia_net_sriov_dp_version }}"
  when:
    - "'primary' in group_names"
    - nvidia_net_operator_apply_nicclusterpolicy | bool
    - _ncp_crd is defined
    - _ncp_crd.resources is defined
    - _ncp_crd.resources | length > 0
    - _ncp_exists is defined
    - _ncp_exists.resources is defined
    - _ncp_exists.resources | length == 0
    - (nvidia_net_operator_missing_nicclusterpolicy_vars | default([])) | length == 0
