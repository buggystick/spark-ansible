apiVersion: v1
kind: ConfigMap
metadata:
  name: trtllm-build-script
  namespace: default
data:
  build.sh: |
        #!/usr/bin/env bash
        set -euo pipefail

        echo "TP=${TP:-1} PP=${PP:-1} WORLD=${WORLD:-1}"
        echo "Models dir: /models"
        nvidia-smi || true

        python3 - <<'PY'
        import json
        import os
        import subprocess
        import sys
        from pathlib import Path

        try:
            from huggingface_hub import snapshot_download
        except Exception:  # pragma: no cover - hub optional inside container
            snapshot_download = None

        models = json.loads(os.environ.get("MODELS_JSON","[]"))
        TP = int(os.environ.get('TP','1'))
        PP = int(os.environ.get('PP','1'))
        WORLD = TP * PP

        def run(cmd):
            print(">>", " ".join(cmd), flush=True)
            r = subprocess.run(cmd)
            if r.returncode != 0:
                sys.exit(r.returncode)

        def resolve_checkpoint(hf_id: str, model_name: str) -> str:
            """Return a local directory containing the HF checkpoint."""
            cache_root = Path(os.environ.get("HF_HOME", Path.home() / ".cache" / "huggingface"))
            cache_root.mkdir(parents=True, exist_ok=True)
            token = os.environ.get("HF_TOKEN") or None

            if snapshot_download is not None:
                return snapshot_download(
                    repo_id=hf_id,
                    cache_dir=str(cache_root),
                    token=token,
                )

            # Fallback to raw cache layout when huggingface_hub is unavailable.
            slug = hf_id.replace("/", "--")
            snapshots_dir = cache_root / "hub" / f"models--{slug}" / "snapshots"
            if not snapshots_dir.is_dir():
                raise RuntimeError(
                    f"Checkpoint for {hf_id} not found at {snapshots_dir}; install huggingface_hub in the TRT-LLM image."
                )
            revisions = sorted(p for p in snapshots_dir.iterdir() if p.is_dir())
            if not revisions:
                raise RuntimeError(f"No snapshots available for {hf_id} in {snapshots_dir}.")
            latest = revisions[-1]
            print(
                f"Using cached Hugging Face snapshot for {model_name} ({hf_id}): {latest}",
                flush=True,
            )
            return str(latest)

        for m in models:
            name = m["name"]
            hf_id = m["hf_id"]
            out = "/models/" + name + "/trtllm"
            os.makedirs(out, exist_ok=True)

            checkpoint_dir = resolve_checkpoint(hf_id, name)
            print(f"Checkpoint directory: {checkpoint_dir}", flush=True)

            cmd = [
                "trtllm-build",
                "--checkpoint_dir",
                checkpoint_dir,
                "--output_dir",
                out,
                "--world_size",
                str(WORLD),
                "--tp_size",
                str(TP),
                "--pp_size",
                str(PP),
                "--gpus_per_node",
                "1",
                "--max_input_len",
                "8192",
                "--max_seq_len",
                str(8192 + 1024),
            ]

            run(cmd)

            lines = []
            lines.append('name: "' + name + '"')
            lines.append('backend: "tensorrtllm"')
            lines.append('max_batch_size: 32')
            lines.append('parameters: { key: "tensor_parallel_size" value: { string_value: "' + str(TP) + '" } }')
            lines.append('parameters: { key: "pipeline_parallel_size" value: { string_value: "' + str(PP) + '" } }')
            lines.append('instance_group [{ kind: KIND_GPU, count: 1 }]')
            cfg = "\n".join(lines) + "\n"

            os.makedirs("/models/" + name, exist_ok=True)
            with open("/models/" + name + "/config.pbtxt", "w") as f:
                f.write(cfg)

            print("== Built " + name + " -> " + out, flush=True)

        print("All builds done.", flush=True)
        PY
