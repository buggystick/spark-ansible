apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: vllm-serve
  namespace: default
spec:
  entrypoint: >
    vllm serve {{ vllm_model | default(models[0].hf_id if models is defined else 'meta-llama/Llama-3.3-70B-Instruct') }}
      --distributed-executor-backend ray
      --ray-address=ray://sparks-ray-head-svc:10001
      --tensor-parallel-size {{ vllm_tensor_parallel_size | default(1) }}
      --pipeline-parallel-size {{ vllm_pipeline_parallel_size | default(2) }}
      --port 8000
      --metrics-port 8080
      --gpu-memory-utilization {{ vllm_gpu_memory_utilization | default(0.9) }}
      --max-num-batched-tokens {{ vllm_max_tokens | default(8192) }}
      --disable-async-io
      --swap-space 8
  runtimeEnv: {}
  shutdownAfterJobFinishes: false
  submissionMode: KUBERNETES
  rayClusterSpec: {}
